---
title: Aula de laboratório - Variáveis Instrumentais
author: Rafael Bressan
date: "2024-09-05"
format: 
  html:
    theme: cosmo
    embed-resources: true
lang: "pt-BR"
engine: jupyter
jupyter: python3
---

## Retornos da educação para mulheres casadas

Vamos analizar os retornos de educação para mulheres casadas com os dados 
`mroz` do pacote `wooldridge`. Filtraremos para usar somente as observações
que onde salário não seja faltante. Como variável instrumental para educação
usaremos a educação do pai. Você acha que este é um bom instrumento? Por quê?


```{python}
import pandas as pd
import wooldridge as woo
import linearmodels.iv as iv
```


```{python}
df = woo.dataWoo('mroz').dropna(subset=['wage'])
```


**Crie uma tabela com as estatíticas descritivas dos dados.** Vamos usar o **Copilot** para criar o código para nós. Uma sugestão de *prompt* é:

| | *Prompt* |
|-|-----|
| ![Copilot](img/copilot-logo.png){width=100%} | `usando a linguagem Python, crie um data frame de estatísticas descritivas para todas as variáveis do data frame df. As estatísticas devem ser número de observações, número de valores únicos, média e desvio padrão.`| 


```{python}
desc = df.describe().loc[["count", "mean", "std"]].transpose()
stats = desc.join(df.nunique().rename("unique"))
stats
```

## Regressão simples

Estimamos primeiro um MQO simples com a função `IV2SLS` **sem utilizar** a especificação de `[endo ~ exog]`.

| | *Prompt* |
|-|-----|
| ![Copilot](img/copilot-logo.png){width=100%} | `estime um modelo linear com a função IV2SLS da biblioteca linearmodels. A variável dependente é o logaritmo do salário (lwage) e a variável independente é o número de anos de educação (educ).`| 


```{python}
ols_reg = iv.IV2SLS.from_formula("lwage ~ 1 + educ", df).fit()
```

Leia a introdução a biblioteca [`linearmodels`](https://bashtage.github.io/linearmodels/iv/introduction.html) e descubra como especificar um modelo de variáveis instrumentais (IVs). 

**Em seguida rode a regressão com `fatheduc` como instrumento para `educ`**, e apresente os resultados das duas regressões lado a lado em uma tabela.

| | *Prompt* |
|-|-----|
| ![Copilot](img/copilot-logo.png){width=100%} | `estime um modelo linear lwage contra educ onde a variavel instrumental para educ será fatheduc. Apresente os resultados das duas regressões lado a lado em uma tabela.`|


```{python}
iv_reg = iv.IV2SLS.from_formula("lwage ~ 1 + [educ ~ fatheduc]", df).fit()
```

```{python}
from linearmodels.iv.results import compare

compare({"OLS": ols_reg, "IV": iv_reg})
```

## Adicionando regressores exógenos

A abordagem de variáveis instrumentais pode facilmente ser estendida para
a inclusão de outros regressores exógenos na regressão.

Inclua em ambas as regressões anteriores os regressores experiência (`exper`) e experiência ao quadrado (`expersq`). Faça também um modelo de MQ2E **manualmente** e apresente os resultados.

| | *Prompt* |
|-|-----|
| ![Copilot](img/copilot-logo.png){width=100%} | `inclua nos dois modelos anteriores os regressores exper e expersq. Em seguida, faça um modelo de minimos quadrados em dois estagios manualmente e apresente os resultados.`|


```{python}
ols_reg = iv.IV2SLS.from_formula("lwage ~ 1 + educ + exper + expersq", df).fit()
iv_reg = iv.IV2SLS.from_formula("lwage ~ 1 + exper + expersq + [educ ~ fatheduc]", df).fit()
stage_1 = iv.IV2SLS.from_formula("educ ~ 1 + exper + expersq + fatheduc", df).fit()
mq2e_df = df.copy()
mq2e_df["pred_educ"] = stage_1.fitted_values
stage_2 = iv.IV2SLS.from_formula("lwage ~ 1 + pred_educ + exper + expersq", mq2e_df).fit()

compare({"OLS": ols_reg, "MQ2E": stage_2, "IV": iv_reg})
```

## Teste de endogeneidade de Hausman

Tanto MQO quanto MQ2E serão consistentes se todas as variáveis forem exógenas. Se os estimadores diferirem de forma significativa, concluímos que o regressor era de fato endógeno (qual hipótese estamos fazendo aqui?).

Portanto, é sempre uma boa ideia calcular tanto o MQO quanto o MQ2E e compará-los. Para determinar se as estimativas são estatisticamente diferentes, então um teste de regressão é mais adequado. O teste de Hausman
faz esse papel.

O teste é separado em dois passos:

1. estime a forma reduzida de variável endógena e salve os resíduos

1. inclua estes resíduos como um regressor adicional na equação estrutural

Um teste t ou F no coeficiente dos resíduos nos fornecerá evidência sobre a
endogeneidade do regressor caso rejeite-se a hipótese nula.

**Implemente o teste de Hausman**


```{python}
mq2e_df["resid_educ"] = stage_1.resids
hausman = iv.IV2SLS.from_formula("lwage ~ 1 + educ + exper + expersq + resid_educ", mq2e_df).fit()
hausman
```

Existe o teste de Wu-Hausman já implementado na biblioteca [`linearmodels`](https://bashtage.github.io/linearmodels/iv/iv/linearmodels.iv.results.IVResults.wu_hausman.html#linearmodels.iv.results.IVResults.wu_hausman). Vamos usar ele para comparar os resultados.

```{python}
iv_reg.wu_hausman(["educ"])
```

## Teste de restrições sobreidentificadoras

Se temos mais instrumentos válidos que variáveis endógenas, a princípio pode-se usar qualquer ou todos os instrumentos. Se todos forem de fato válidos, então, usar todos trará ganhos de eficiência no estimador MQ2E.

1. Estime a equação estrutural por MQ2E, usando mais instrumentos que variáveis endógenas, e salve os resíduos $\hat u_1$

2. Regrida $\hat u_1$ em todas as variáveis exógenas e salve o $R^2$ desta regressão

3. Sob a hipótese nula de que todas as VI são exógenas, a estatística de teste $n R^2$ é assintoticamente distribuída como uma Qui-quadrado com $q$ graus de
liberdade, onde $q$ é a diferença entre o número de instrumentos e de variáveis endógenas.

**Faça o teste de restrições sobreidentificadoras considerando que a educação da mãe (`motheduc`) também é um instrumento váldio para educação.**

```{python}
sobre_iv = iv.IV2SLS.from_formula(
    "lwage ~ 1 + exper + expersq [educ ~ fatheduc + motheduc]", df
).fit()
sobre_df = df.copy()
sobre_df["resid"] = sobre_iv.resids
resid_reg = iv.IV2SLS.from_formula(
    "resid ~ 1 + exper + expersq + fatheduc + motheduc", sobre_df
).fit()
n = resid_reg.nobs
r2 = resid_reg.rsquared
sobre_stat = n * r2
```

```{python}
# Import the chi2 distribution
from scipy.stats import chi2

# Calculate the p-value
p_value = 1 - chi2.cdf(sobre_stat, 1)
print(f"Estatística de teste: {sobre_stat}")
print(f"P-valor: {p_value}")
```

Ou podemos usar o teste de Wooldridge para sobreidentificação.

```{python}
sobre_iv.wooldridge_overid
```